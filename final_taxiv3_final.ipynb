{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM449/mhikhBCYWJ6unr2R+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikohuhu/q-learning-taxi-v3-MMAI845/blob/JAY/final_taxiv3_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ACYemFgvGGIA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0kQ9SMAZ_7Hd"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variables"
      ],
      "metadata": {
        "id": "wgMyo6TCF-Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "NUM_EPISODES = 100000\n",
        "NUM_EVAL_EPISODES = 100"
      ],
      "metadata": {
        "id": "uPZ0G1_YAHmo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample"
      ],
      "metadata": {
        "id": "H5R0BKgnF8U_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(env, num_episodes):\n",
        "    total_penalties = 0\n",
        "    total_timesteps = 0\n",
        "    total_rewards = 0\n",
        "    start_time = time.time()  # Start time\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        timesteps = 0\n",
        "        penalties = 0\n",
        "        episode_rewards = 0\n",
        "\n",
        "        while not done:\n",
        "            action = env.action_space.sample()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            timesteps += 1\n",
        "            episode_rewards += reward\n",
        "\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "        total_timesteps += timesteps\n",
        "        total_penalties += penalties\n",
        "        total_rewards += episode_rewards\n",
        "\n",
        "    average_penalties = total_penalties / num_episodes\n",
        "    average_timesteps = total_timesteps / num_episodes\n",
        "    average_rewards_per_move = total_rewards / (total_timesteps - total_penalties)  # Average rewards per move\n",
        "\n",
        "    end_time = time.time()  # End time\n",
        "    execution_time = end_time - start_time  # Calculate execution time\n",
        "\n",
        "    return average_penalties, average_timesteps, average_rewards_per_move, execution_time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "    NUM_EPISODES = 100000\n",
        "    avg_penalties, avg_timesteps, avg_rewards_per_move, execution_time = sampling(env, NUM_EPISODES)\n",
        "    print(\"Sampling Method:\")\n",
        "    print(\"Average timesteps per trip: {}\".format(avg_timesteps))\n",
        "    print(\"Average penalties per episode: {}\".format(avg_penalties))\n",
        "    print(\"Average rewards per move: {}\".format(avg_rewards_per_move))\n",
        "    print(\"Execution time: {:.2f} seconds\".format(execution_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-q_0xmRAX6e",
        "outputId": "0c1fb917-9eee-492e-eb7b-13a3dddd772a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling Method:\n",
            "Average timesteps per trip: 196.64697\n",
            "Average penalties per episode: 63.97952\n",
            "Average rewards per move: -5.815110337916346\n",
            "Execution time: 676.15 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning"
      ],
      "metadata": {
        "id": "qRMurZZYGjzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "def epsilon_greedy_policy(Q, state, epsilon):\n",
        "    if np.random.uniform(0, 1) > epsilon:\n",
        "        action = np.argmax(Q[state])\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "    return action\n",
        "\n",
        "def train_q_learning(Q, env, num_episodes, alpha, gamma, epsilon):\n",
        "    start_time = time.time()\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_max_q_value = np.max(Q[next_state]) if not done else 0\n",
        "            Q[state][action] += alpha * (reward + gamma * next_max_q_value - Q[state][action])\n",
        "\n",
        "            state = next_state\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    return execution_time\n",
        "\n",
        "\n",
        "def evaluate_q_learning(Q, env, num_episodes):\n",
        "    total_penalties = 0\n",
        "    total_timesteps = 0\n",
        "    total_rewards = 0\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        penalties = 0\n",
        "        timesteps = 0\n",
        "        rewards = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(Q[state])\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            rewards += reward\n",
        "            timesteps += 1\n",
        "\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "        total_penalties += penalties\n",
        "        total_timesteps += timesteps\n",
        "        total_rewards += rewards\n",
        "\n",
        "    average_penalties = total_penalties / num_episodes\n",
        "    average_timesteps = total_timesteps / num_episodes\n",
        "    average_rewards = total_rewards / num_episodes\n",
        "\n",
        "    print(\"Q-learning Method:\")\n",
        "    print(\"Average number of penalties per episode:\", average_penalties)\n",
        "    print(\"Average number of timesteps per trip:\", average_timesteps)\n",
        "    print(\"Average rewards per move:\", average_rewards)\n",
        "\n",
        "def main():\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "    NUM_EPISODES = 100000\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    alpha = 0.1\n",
        "    gamma = 0.6\n",
        "    epsilon = 0.1\n",
        "\n",
        "    execution_time = train_q_learning(Q, env, NUM_EPISODES, alpha, gamma, epsilon)\n",
        "    evaluate_q_learning(Q, env, 100)\n",
        "    print(\"Execution time: {:.2f} seconds\".format(execution_time))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFlCFuTSAqYd",
        "outputId": "099e367b-7e87-4725-d8ac-41b1bfa9da5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-learning Method:\n",
            "Average number of penalties per episode: 0.0\n",
            "Average number of timesteps per trip: 12.86\n",
            "Average rewards per move: 8.14\n",
            "Execution time: 81.18 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARSA"
      ],
      "metadata": {
        "id": "Q6jBio3kGtOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Q, state, epsilon, env):\n",
        "    if np.random.uniform(0, 1) > epsilon:\n",
        "        action = np.argmax(Q[state])\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "    return action\n",
        "\n",
        "def train_sarsa(Q, env, num_episodes, alpha, gamma, epsilon):\n",
        "    start_time = time.time()\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        action = epsilon_greedy_policy(Q, state, epsilon, env)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_action = epsilon_greedy_policy(Q, next_state, epsilon, env)\n",
        "\n",
        "            next_q_value = Q[next_state][next_action] if not done else 0\n",
        "            Q[state][action] += alpha * (reward + gamma * next_q_value - Q[state][action])\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    return execution_time\n",
        "\n",
        "def evaluate_sarsa(Q, env, num_episodes):\n",
        "    total_penalties = 0\n",
        "    total_timesteps = 0\n",
        "    total_rewards = 0\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        penalties = 0\n",
        "        timesteps = 0\n",
        "        rewards = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(Q[state])\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            rewards += reward\n",
        "            timesteps += 1\n",
        "\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "        total_penalties += penalties\n",
        "        total_timesteps += timesteps\n",
        "        total_rewards += rewards\n",
        "\n",
        "    average_penalties = total_penalties / num_episodes\n",
        "    average_timesteps = total_timesteps / num_episodes\n",
        "    average_rewards = total_rewards / num_episodes\n",
        "\n",
        "    print(\"SARSA Method:\")\n",
        "    print(\"Average number of penalties per episode:\", average_penalties)\n",
        "    print(\"Average number of timesteps per trip:\", average_timesteps)\n",
        "    print(\"Average rewards per move:\", average_rewards)\n",
        "\n",
        "def main():\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "    NUM_EPISODES = 100000\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    alpha = 0.1\n",
        "    gamma = 0.6\n",
        "    epsilon = 0.1\n",
        "\n",
        "    print(\"\\nRunning SARSA...\")\n",
        "    execution_time = train_sarsa(Q, env, NUM_EPISODES, 0.2, 0.8, 0.1)\n",
        "    evaluate_sarsa(Q, env, 100)\n",
        "    print(\"Execution time: {:.2f} seconds\".format(execution_time))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohdcrZqiEEaN",
        "outputId": "6703dfe2-9be3-446e-9133-fc2eb844e96a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running SARSA...\n",
            "SARSA Method:\n",
            "Average number of penalties per episode: 0.0\n",
            "Average number of timesteps per trip: 28.55\n",
            "Average rewards per move: -9.23\n",
            "Execution time: 117.97 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-Learning"
      ],
      "metadata": {
        "id": "8Q_3-EwAGzlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from keras.optimizers import Adam\n",
        "import gym\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Reshape\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm  # Importing tqdm for progress bar\n",
        "\n",
        "# Initialize the environment with the new step API\n",
        "env_taxi = gym.make(\"Taxi-v3\", new_step_api=True).env\n",
        "\n",
        "class TaxiAgent:\n",
        "    def __init__(self, env, optimizer):\n",
        "        self._state_size = env.observation_space.n\n",
        "        self._action_size = env.action_space.n\n",
        "        self._optimizer = optimizer\n",
        "        self.experience_replay_memory = deque(maxlen=2000)\n",
        "        self.discount = 0.6\n",
        "        self.exploration = 0.1\n",
        "        self.q_network = self._build_compile_model()\n",
        "        self.target_network = self._build_compile_model()\n",
        "        self.align_both_model()\n",
        "\n",
        "    def gather(self, state, action, reward, next_state, terminated):\n",
        "        self.experience_replay_memory.append((state, action, reward, next_state, terminated))\n",
        "\n",
        "    def _build_compile_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self._state_size, 10, input_length=1))\n",
        "        model.add(Reshape((10,)))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(self._action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=self._optimizer)\n",
        "        return model\n",
        "\n",
        "    def align_both_model(self):\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.exploration:\n",
        "            return env_taxi.action_space.sample()\n",
        "        q_values = self.q_network.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def retrain(self, batch_size):\n",
        "        minibatch = random.sample(self.experience_replay_memory, batch_size)\n",
        "        for state, action, reward, next_state, terminated in minibatch:\n",
        "            target = self.q_network.predict(state, verbose=0)\n",
        "            if terminated:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                t = self.target_network.predict(next_state, verbose=0)\n",
        "                target[0][action] = reward + self.discount * np.amax(t)\n",
        "            self.q_network.fit(state, target, epochs=1, verbose=0)  # Reduce the epochs\n",
        "\n",
        "def deep_q_learning(env, num_training_episodes=100, num_evaluation_episodes=100, alpha=0.01, gamma=0.6, epsilon=0.1, batch_size=32, timesteps_per_episode=40, epochs=4):\n",
        "    # Training phase\n",
        "    start_time = time.time()  # Record start time for training\n",
        "    total_penalties = 0\n",
        "    total_timesteps = 0\n",
        "    total_rewards = 0\n",
        "\n",
        "    for e in tqdm(range(num_training_episodes), desc=\"Training Episodes\"):\n",
        "        state = env_taxi.reset()\n",
        "        state = np.reshape(state, [1, 1])\n",
        "        reward = 0\n",
        "        terminated = False\n",
        "        timesteps = 0\n",
        "        penalties = 0\n",
        "\n",
        "        while not terminated and timesteps < timesteps_per_episode:\n",
        "            action = taxi_agent.act(state)\n",
        "            next_state, reward, terminated, truncated, info = env_taxi.step(action)\n",
        "            next_state = np.reshape(next_state, [1, 1])\n",
        "            taxi_agent.gather(state, action, reward, next_state, terminated)\n",
        "            state = next_state\n",
        "            timesteps += 1\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "            # Debugging print statements\n",
        "            print(f\"Episode {e + 1}, Timestep {timesteps}: action={action}, reward={reward}, terminated={terminated}, penalties={penalties}\")\n",
        "\n",
        "            if len(taxi_agent.experience_replay_memory) > batch_size and timesteps % 10 == 0:  # Retrain less frequently\n",
        "                print(f\"Retraining on mini-batch at episode {e + 1}\")\n",
        "                taxi_agent.retrain(batch_size)\n",
        "\n",
        "        total_penalties += penalties\n",
        "        total_timesteps += timesteps\n",
        "        total_rewards += reward\n",
        "\n",
        "    end_time = time.time()  # Record end time for training\n",
        "    execution_time = end_time - start_time  # Calculate time to train\n",
        "\n",
        "    # Evaluation phase\n",
        "    total_penalties = 0\n",
        "    total_timesteps = 0\n",
        "    total_rewards = 0\n",
        "    for e in tqdm(range(num_evaluation_episodes), desc=\"Evaluation Episodes\"):\n",
        "        # Evaluation episode loop\n",
        "        state = env_taxi.reset()\n",
        "        state = np.reshape(state, [1, 1])\n",
        "        reward = 0\n",
        "        terminated = False\n",
        "        timesteps = 0\n",
        "        penalties = 0\n",
        "\n",
        "        while not terminated and timesteps < timesteps_per_episode:\n",
        "            action = taxi_agent.act(state)\n",
        "            next_state, reward, terminated, truncated, info = env_taxi.step(action)\n",
        "            next_state = np.reshape(next_state, [1, 1])\n",
        "            state = next_state\n",
        "            timesteps += 1\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "            # Debugging print statements\n",
        "            print(f\"Evaluation Episode {e + 1}, Timestep {timesteps}: action={action}, reward={reward}, terminated={terminated}, penalties={penalties}\")\n",
        "\n",
        "        total_penalties += penalties\n",
        "        total_timesteps += timesteps\n",
        "        total_rewards += reward\n",
        "\n",
        "    # Calculate averages for evaluation metrics\n",
        "    average_penalties = total_penalties / num_evaluation_episodes\n",
        "    average_timesteps = total_timesteps / num_evaluation_episodes\n",
        "    average_rewards_per_move = total_rewards / (total_timesteps - total_penalties)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Deep Q-Learning Method:\")\n",
        "    print(\"Average number of penalties per episode:\", average_penalties)\n",
        "    print(\"Average number of timesteps per trip:\", average_timesteps)\n",
        "    print(\"Average rewards per move:\", average_rewards_per_move)\n",
        "    print(\"Time to train:\", execution_time, \"seconds\")\n",
        "\n",
        "# Creating the optimizer\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "# Creating the TaxiAgent instance\n",
        "taxi_agent = TaxiAgent(env_taxi, optimizer)\n",
        "\n",
        "# Defining parameters for the experiment\n",
        "num_training_episodes = 100\n",
        "num_evaluation_episodes = 100\n",
        "batch_size = 32\n",
        "timesteps_per_episode = 40\n",
        "\n",
        "# Running the Deep Q-Learning experiment\n",
        "deep_q_learning(env_taxi, num_training_episodes, num_evaluation_episodes, batch_size=batch_size, timesteps_per_episode=timesteps_per_episode)\n"
      ],
      "metadata": {
        "id": "rq6IVR5tFkHv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table"
      ],
      "metadata": {
        "id": "KBnK7wAZGW8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm_data = {\n",
        "    \"Metric\": [\"Average Penalties\", \"Average Timesteps\", \"Average Rewards per Move\", \"Training Execution Time (seconds)\"],\n",
        "    \"Sample Method\": [63.97952, 196.64697, -5.815110337916346, 676.15]\n",
        "}\n",
        "\n",
        "ql_data = {\n",
        "    \"Metric\": [\"Average Penalties\", \"Average Timesteps\", \"Average Rewards per Move\", \"Training Execution Time (seconds)\"],\n",
        "    \"Q-Learning\": [0.0, 12.86, 8.14, 81.18]\n",
        "}\n",
        "\n",
        "sar_data = {\n",
        "    \"Metric\": [\"Average Penalties\", \"Average Timesteps\", \"Average Rewards per Move\", \"Training Execution Time (seconds)\"],\n",
        "    \"SARSA\": [0.0, 28.55, -9.23, 117.97]\n",
        "}\n",
        "\n",
        "dql_data = {\n",
        "    \"Metric\": [\"Average Penalties\", \"Average Timesteps\", \"Average Rewards per Move\", \"Training Execution Time (seconds)\"],\n",
        "    \"Deep Q-Learning\": [1.4, 40.0, -0.03290155440414508, 3528.5490431785583]\n",
        "}\n",
        "# These two are not learning the right thing to do, they are learning what not to do in that they are learning to minimize penalties vs. trying to maximize rewards\n",
        "\n",
        "# Create DataFrames\n",
        "sm_df = pd.DataFrame(sm_data)\n",
        "ql_df = pd.DataFrame(ql_data)\n",
        "sarsa_df = pd.DataFrame(sar_data)\n",
        "dql_df = pd.DataFrame(dql_data)\n",
        "\n",
        "# Merge DataFrames\n",
        "summary_df = pd.merge(sm_df, ql_df, on=\"Metric\")\n",
        "summary_df = pd.merge(summary_df, sarsa_df, on=\"Metric\")\n",
        "summary_df = pd.merge(summary_df, dql_df, on=\"Metric\")\n",
        "summary_df = summary_df.round(2)\n",
        "\n",
        "print(\"Summary DataFrame:\")\n",
        "summary_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "DB26tq7fFr9l",
        "outputId": "2890ce14-aeec-4a81-fd8d-ac2606cbd23f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary DataFrame:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              Metric  Sample Method  Q-Learning   SARSA  \\\n",
              "0                  Average Penalties          63.98        0.00    0.00   \n",
              "1                  Average Timesteps         196.65       12.86   28.55   \n",
              "2           Average Rewards per Move          -5.82        8.14   -9.23   \n",
              "3  Training Execution Time (seconds)         676.15       81.18  117.97   \n",
              "\n",
              "   Deep Q-Learning  \n",
              "0             1.40  \n",
              "1            40.00  \n",
              "2            -0.03  \n",
              "3          3528.55  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02926836-8436-4af1-93e4-3392b2d4035e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Sample Method</th>\n",
              "      <th>Q-Learning</th>\n",
              "      <th>SARSA</th>\n",
              "      <th>Deep Q-Learning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Average Penalties</td>\n",
              "      <td>63.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Average Timesteps</td>\n",
              "      <td>196.65</td>\n",
              "      <td>12.86</td>\n",
              "      <td>28.55</td>\n",
              "      <td>40.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average Rewards per Move</td>\n",
              "      <td>-5.82</td>\n",
              "      <td>8.14</td>\n",
              "      <td>-9.23</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Training Execution Time (seconds)</td>\n",
              "      <td>676.15</td>\n",
              "      <td>81.18</td>\n",
              "      <td>117.97</td>\n",
              "      <td>3528.55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02926836-8436-4af1-93e4-3392b2d4035e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02926836-8436-4af1-93e4-3392b2d4035e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02926836-8436-4af1-93e4-3392b2d4035e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4c0db23e-0ab0-43c4-8b0e-cc7a43d937b3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c0db23e-0ab0-43c4-8b0e-cc7a43d937b3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4c0db23e-0ab0-43c4-8b0e-cc7a43d937b3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c947d343-4714-4423-9dd9-c470dcb6f6f0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c947d343-4714-4423-9dd9-c470dcb6f6f0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "summary_df",
              "summary": "{\n  \"name\": \"summary_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Average Timesteps\",\n          \"Training Execution Time (seconds)\",\n          \"Average Penalties\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sample Method\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 307.30317679234406,\n        \"min\": -5.82,\n        \"max\": 676.15,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          196.65,\n          676.15,\n          63.98\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q-Learning\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.46840224331252,\n        \"min\": 0.0,\n        \"max\": 81.18,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          12.86,\n          81.18,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SARSA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 58.03757482585456,\n        \"min\": -9.23,\n        \"max\": 117.97,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          28.55,\n          117.97,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Deep Q-Learning\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1757.477819850557,\n        \"min\": -0.03,\n        \"max\": 3528.55,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          40.0,\n          3528.55,\n          1.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}